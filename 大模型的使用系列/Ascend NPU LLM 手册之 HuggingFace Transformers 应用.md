- [ç®€ä»‹](#ç®€ä»‹)
- [ChatGLM2-6B æ¨ç†](#chatglm2-6b-æ¨ç†)
    - [å®˜æ–¹ç‰ˆæœ¬ï¼ˆè„šæœ¬ï¼‰](#å®˜æ–¹ç‰ˆæœ¬è„šæœ¬)
    - [å®˜æ–¹ç‰ˆæœ¬ï¼ˆäº¤äº’å¼ç•Œé¢ï¼‰](#å®˜æ–¹ç‰ˆæœ¬äº¤äº’å¼ç•Œé¢)
    - [å¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°ï¼ˆè„šæœ¬ï¼‰](#å¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°è„šæœ¬)
    - [å¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°ï¼ˆäº¤äº’å¼ç•Œé¢ï¼‰](#å¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°äº¤äº’å¼ç•Œé¢)
- [ChatGLM2-6B è®­ç»ƒ](#chatglm2-6b-è®­ç»ƒ)
- [é—®é¢˜](#é—®é¢˜)

### ç®€ä»‹

HuggingFace Transformers æä¾›äº†å¯ä»¥è½»æ¾åœ°ä¸‹è½½å¹¶ä¸”è®­ç»ƒå…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹çš„ API å’Œå·¥å…·ã€‚
è¿™äº›æ¨¡å‹æ”¯æŒä¸åŒæ¨¡æ€ä¸­çš„å¸¸è§ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š

ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ã€è¯­è¨€å»ºæ¨¡ã€æ‘˜è¦ã€ç¿»è¯‘ã€å¤šé¡¹é€‰æ‹©å’Œæ–‡æœ¬ç”Ÿæˆã€‚
ğŸ–¼ï¸ æœºå™¨è§†è§‰ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ã€‚
ğŸ—£ï¸ éŸ³é¢‘ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘åˆ†ç±»ã€‚
ğŸ™ å¤šæ¨¡æ€ï¼šè¡¨æ ¼é—®ç­”ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€ä»æ‰«ææ–‡æ¡£æå–ä¿¡æ¯ã€è§†é¢‘åˆ†ç±»å’Œè§†è§‰é—®ç­”ã€‚

å®˜æ–¹ Transoformers æ”¯æŒåœ¨ PyTorchã€TensorFlowã€JAXä¸Šæ“ä½œï¼Œæ˜‡è…¾å½“å‰å·²å®Œæˆ Transformers çš„åŸç”Ÿæ”¯æŒï¼Œæœ¬æ–‡æ¡£å°†ä¼šæ‰‹æŠŠæ‰‹å¸¦é¢†å¤§å®¶åœ¨æ˜‡è…¾ä¸Šä½¿ç”¨ Transformers æ¥ç©è½¬å¤§æ¨¡å‹ã€‚

**å‰æï¼šç¡®ä¿å·²å®Œæˆ [Ascend NPU ä¸¹ç‚‰æ­å»º](https://zhuanlan.zhihu.com/p/681513155)ã€‚**

### ChatGLM2-6B æ¨ç†

ä¸‹é¢æ¼”ç¤ºå‡ ç§æ¨ç†æ–¹å¼ï¼ŒåŒ…æ‹¬ ChatGLM2-6B å®˜æ–¹ç‰ˆæœ¬åŠå¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°ï¼Œå¯ä»¥è„šæœ¬æ–¹å¼æˆ–äº¤äº’å¼ç•Œé¢æ–¹å¼ä½¿ç”¨ã€‚

##### å®˜æ–¹ç‰ˆæœ¬ï¼ˆè„šæœ¬ï¼‰

å½“å‰ ChatGLM2-6B æ¨¡å‹å·²åšåˆ°æ˜‡è…¾åŸç”Ÿæ”¯æŒï¼Œæ‰€ä»¥ç›´æ¥å‚è€ƒ ChatGLM2-6B å®˜æ–¹æ•™ç¨‹å³å¯ã€‚

é¦–å…ˆè¿›è¡Œç¯å¢ƒå®‰è£…ï¼š

```shell
# ä¸‹è½½è„šæœ¬
git clone https://github.com/THUDM/ChatGLM2-6B
cd ChatGLM2-6B
# ä¸‹è½½ä¾èµ–
pip install -r requirements.txt
```

ä¹‹åè¿›è¡Œè„šæœ¬æ¨ç†ï¼š

```python
from transformers import AutoTokenizer, AutoModel
# è‹¥æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨å›½å†…é•œåƒç½‘ç«™è¿›è¡Œæ¨¡å‹çš„ä¸‹è½½ï¼Œå¹¶å°†ä¸Šè¿°ä»£ç ä¸­æ¨¡å‹è·¯å¾„æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True, device='npu')
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
response, history = model.chat(tokenizer, "NPUå’ŒGPUæœ‰ä»€ä¹ˆåŒºåˆ«", history=history)
print(response)
```
response = model.chat(tokenizer, [{"role": "user", "content": "ä½ å¥½"}])
Output:

```shell
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:29<00:00,  4.15s/it]
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
NPUï¼ˆç¥ç»å¤„ç†å™¨ï¼‰å’ŒGPUï¼ˆå›¾å½¢å¤„ç†å™¨ï¼‰éƒ½æ˜¯ä¸“é—¨ä¸ºåŠ é€Ÿæ·±åº¦å­¦ä¹ è®¡ç®—è€Œè®¾è®¡çš„å¤„ç†å™¨ã€‚å®ƒä»¬ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š

1. æ¶æ„ï¼šNPU å’Œ GPU çš„æ¶æ„æœ‰å¾ˆå¤§çš„ä¸åŒã€‚NPU é‡‡ç”¨å…¨æ–°çš„èŠ¯ç‰‡æ¶æ„ï¼Œä¸“ä¸ºæ·±åº¦å­¦ä¹ è®¡ç®—è€Œè®¾è®¡ï¼Œè€Œ GPU åˆ™åŸºäºä¼ ç»Ÿçš„å›¾å½¢æ¸²æŸ“æ¶æ„ã€‚

2. æ€§èƒ½ï¼šç”±äº NPU çš„æ¶æ„ä¸“é—¨ä¸ºæ·±åº¦å­¦ä¹ è®¡ç®—è€Œè®¾è®¡ï¼Œå› æ­¤åœ¨è¿›è¡Œæ·±åº¦å­¦ä¹ è®¡ç®—æ—¶ï¼ŒNPU å¾€å¾€å…·æœ‰æ›´å¼ºå¤§çš„æ€§èƒ½ä¼˜åŠ¿ã€‚ä¸ GPU ç›¸æ¯”ï¼ŒNPU åœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ˆå¦‚å¤§è§„æ¨¡æ•´æ•°è®¡ç®—å’ŒçŸ©é˜µè¿ç®—ï¼‰çš„æ€§èƒ½å¯èƒ½ç•¥é€Šä¸€ç­¹ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒNPU é€šå¸¸èƒ½æä¾›æ›´é«˜çš„æ€§èƒ½ã€‚

3. èƒ½æ•ˆæ¯”ï¼šåœ¨ç›¸åŒæ€§èƒ½çš„æƒ…å†µä¸‹ï¼ŒNPU çš„èƒ½æ•ˆæ¯” GPU æ›´é«˜ã€‚è¿™æ„å‘³ç€ NPU å¯ä»¥åœ¨æ›´çŸ­çš„æ—¶é—´å†…å®Œæˆæ·±åº¦å­¦ä¹ è®¡ç®—ï¼Œå¹¶ä¸”åœ¨ä¸æ˜¾è‘—å¢åŠ ç¡¬ä»¶æˆæœ¬çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„æ€§èƒ½ã€‚

4. è½¯ä»¶æ”¯æŒï¼šGPU æ‹¥æœ‰æ›´å¹¿æ³›çš„è½¯ä»¶æ”¯æŒå’Œæ›´æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿã€‚è®¸å¤šæµè¡Œçš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½å·²ç»æ”¯æŒ GPUï¼ŒåŒæ—¶ GPU ä¹Ÿæ˜¯è®¸å¤šå¤§å‹äº‘è®¡ç®—å¹³å°çš„é»˜è®¤åŠ é€Ÿå™¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒNPU çš„ç”Ÿæ€ç³»ç»Ÿç›¸å¯¹è¾ƒæ–°ï¼Œä½† NPU çš„æ”¯æŒå¯¹äºæŸäº›ç‰¹å®šçš„æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½å¯èƒ½æ›´åŠ ä¾¿æ·ã€‚

5. ä»·æ ¼ï¼šç”±äº NPU é‡‡ç”¨å…¨æ–°çš„èŠ¯ç‰‡æ¶æ„ï¼Œå¹¶ä¸” NUPro å¤„ç†å™¨åœ¨å¸‚åœºä¸Šçš„åº”ç”¨å°šä¸å¹¿æ³›ï¼Œå› æ­¤ NPU çš„ä»·æ ¼ç›¸å¯¹è¾ƒé«˜ã€‚ç„¶è€Œï¼Œéšç€ NPU çš„åº”ç”¨åœºæ™¯è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå…¶ä»·æ ¼å¯èƒ½ä¼šé€æ¸é™ä½ã€‚

æ€»ç»“ï¼šåœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­ï¼ŒNPU å’Œ GPU éƒ½å¯ä»¥ç”¨äºåŠ é€Ÿè®¡ç®—ã€‚ç„¶è€Œï¼Œåœ¨ç‰¹å®šçš„ä»»åŠ¡ä¸­ï¼ŒNPU å¯èƒ½å…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œè€Œåœ¨èƒ½æ•ˆæ¯”æ–¹é¢ï¼ŒNPU é€šå¸¸ä¼šæ›´å…·ä¼˜åŠ¿ã€‚åœ¨é€‰æ‹©ä½¿ç”¨å“ªç§å¤„ç†å™¨æ—¶ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œéœ€æ±‚æ¥ç»¼åˆè€ƒè™‘ã€‚
```

##### å®˜æ–¹ç‰ˆæœ¬ï¼ˆäº¤äº’å¼ç•Œé¢ï¼‰

é™¤äº†è„šæœ¬æ–¹å¼ï¼Œå®˜æ–¹æä¾›äº†æ›´æ–¹ä¾¿çš„ç•Œé¢äº¤äº’æ–¹å¼ã€‚

ä¿®æ”¹ `web_demo.py` ä»£ç ä¸­æ¨¡å‹è®¾å¤‡ç±»å‹ï¼š`model = AutoModel.from_pretrained("THUDM/chatglm2-6b/", trust_remote_code=True, device="npu")`ã€‚

ç”±äº CANN å½“å‰åœ¨çº¿ç¨‹é—´æ— æ³•å…±äº« `context`ï¼Œéœ€è¦åœ¨æœ¬åœ°ä¸‹è½½çš„ ChatGLM2-6B æ¨¡å‹è·¯å¾„ä¸‹ï¼Œæˆ–è€… HuggingFace ç¼“å­˜ ChatGLM2-6B æ¨¡å‹è·¯å¾„ä¸‹å¢åŠ å¦‚ä¸‹ä»£ç ï¼ˆå¾…ä¿®å¤ååˆ é™¤ï¼‰ï¼š

```diff
diff --git a/modeling_chatglm.py b/modeling_chatglm.py
index d3fb395..5343d30 100644
--- a/modeling_chatglm.py
+++ b/modeling_chatglm.py
@@ -1016,10 +1018,14 @@ class ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel):
         else:
             prompt = "[Round {}]\n\n<E9><97><AE><EF><BC><9A>{}\n\n<E7><AD><94><EF><BC><9A>".format(len(history) + 1, query)
             inputs = tokenizer([prompt], return_tensors="pt")
+        import torch
+        torch.npu.set_device(0)
         inputs = inputs.to(self.device)
```

å¯åŠ¨å‘½ä»¤ï¼š `python web_demo.py`

ç•Œé¢æ•ˆæœï¼š

![Alt text](https://raw.githubusercontent.com/wangshuai09/blog_img/main/images/20240205163404.png)


##### å¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°ï¼ˆè„šæœ¬ï¼‰

å¤§æ¨¡å‹è®­æ¨å¹³å°ï¼Œä¾‹å¦‚ [FastChat](https://github.com/lm-sys/FastChat)ã€[FlagAI](https://github.com/FlagAI-Open/FlagAI) ç­‰ï¼ŒæŠ½è±¡å‡ºå¤§æ¨¡å‹çš„è®­ç»ƒã€æ¨ç†é€»è¾‘ï¼Œæ”¯æŒå¤šç§å¤šæ ·çš„å¤§æ¨¡å‹ï¼Œæ–¹ä¾¿å¼€å‘è€…çš„ä½¿ç”¨ã€‚

å‚è€ƒä¸‹é¢æ­¥éª¤ï¼Œå¯ä»¥ä½¿ç”¨ FastChat è¿›è¡Œ ChatGLM2-6B çš„æ¨ç†ã€‚

FastChat ç¯å¢ƒå®‰è£…ï¼š

```shell
# 1. æºç å®‰è£…
git clone https://github.com/lm-sys/FastChat.git
cd FastChat
pip3 install --upgrade pip  # enable PEP 660 support
pip3 install -e ".[model_worker,webui]"
# 2. pip å®‰è£…
pip3 install "fschat[model_worker,webui]"
```

æ¨ç†æ­¥éª¤ï¼š

```shell
root@ascend-01:/home/downloads# python -m fastchat.serve.cli --model-path /home/models/chatglm2-6b/ --device npu --temperature 1e-6 
/root/miniconda/envs/model_run/lib/python3.9/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.72it/s]
é—®: ä½ å¥½
ç­”: ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
é—®: ä½ æ˜¯è°
ç­”: æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œç”±æ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº2023å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹è®­ç»ƒè€Œæˆã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚
```

##### å¤§æ¨¡å‹è®­æ¨ä¸€ä½“å¹³å°ï¼ˆäº¤äº’å¼ç•Œé¢ï¼‰

åˆ†åˆ«äºä¸‰ä¸ª shell çª—å£æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```shell
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path /home/models/chatglm2-6b/ --device npu
python3 -m fastchat.serve.gradio_web_server
```

ä¹‹åæ‰“å¼€æµè§ˆå™¨è¾“å…¥ `http://x.x.x.x:7860/`, x.x.x.x ä¸ºå¯åŠ¨æœåŠ¡æœºå™¨ ip åœ°å€ï¼Œç»“æœå¦‚ä¸‹ï¼š

![Alt text](https://raw.githubusercontent.com/wangshuai09/blog_img/main/images/20240205163957.png)

### ChatGLM2-6B è®­ç»ƒ

ä½¿ç”¨ FastChat è¿›è¡Œ ChatGLM2-6B çš„è®­ç»ƒï¼Œå®˜æ–¹ç‰ˆæœ¬æš‚æœªå°è¯•ã€‚

æ‰§è¡Œå¦‚ä¸‹è„šæœ¬å³å¯å¼€å§‹æ¨¡å‹çš„è®­ç»ƒï¼Œæ­¤å¤„æ•°æ®é›†ä½¿ç”¨ HuggingFace çš„ä¸­æ–‡æ•°æ®é›† [FreedomIntelligence/evol-instruct-chinese](https://huggingface.co/datasets/FreedomIntelligence/evol-instruct-chinese), ä¸ºåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼ŒæŠ½å‡ºä¸€ä¸ªå­é›†è¿›è¡Œè®­ç»ƒï¼š

```shell
cd FastChat
torchrun --nproc_per_node=2 --master_port=20001 fastchat/train/train.py \
    --model_name_or_path /home/models/chatglm2-6b \
    --data_path /home/datasets/evol-instruct-chinese/evol-instruct-chinese_subset.json \
    --fp16 True \
    --output_dir output_chatglm \
    --num_train_epochs 5 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "epoch" \
    --learning_rate 5e-5 \
    --weight_decay 0. \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --model_max_length 512 \
    --gradient_checkpointing True \
    --lazy_preprocess True \
    --trust_remote_code True \
    --padding_side left
```

è®­ç»ƒè¿‡ç¨‹æ—¥å¿—ï¼š

```shell
{'loss': 2.0576, 'learning_rate': 4.9995181012051625e-05, 'epoch': 0.03}
  1%|          | 1/160 [02:51<6:06:55, 138.46s/it]WARNING: tokenization mismatch: 251 vs. 253. #turn = 1. (ignored)
WARNING: tokenization mismatch: 295 vs. 297. #turn = 1. (ignored)
{'loss': 1.5809, 'learning_rate': 4.9980725906018074e-05, 'epoch': 0.06}
  1%|â–         | 2/160 [04:25<5:46:37, 131.63s/it]WARNING: tokenization mismatch: 165 vs. 167. #turn = 1. (ignored)
WARNING: tokenization mismatch: 240 vs. 242. #turn = 1. (ignored)
{'loss': 1.4731, 'learning_rate': 4.9956640254617906e-05, 'epoch': 0.09}
  2%|â–         | 3/160 [05:54<4:53:51, 112.30s/it]WARNING: tokenization mismatch: 240 vs. 242. #turn = 1. (ignored)
WARNING: tokenization mismatch: 179 vs. 181. #turn = 1. (ignored)
{'loss': 1.1182, 'learning_rate': 4.99229333433282e-05, 'epoch': 0.12}
  2%|â–         | 4/160 [07:24<4:28:52, 103.41s/it]WARNING: tokenization mismatch: 290 vs. 292. #turn = 1. (ignored)
WARNING: tokenization mismatch: 212 vs. 214. #turn = 1. (ignored)
{'loss': 1.376, 'learning_rate': 4.987961816680492e-05, 'epoch': 0.16}
  3%|â–         | 5/160 [08:50<4:11:15, 97.26s/it]WARNING: tokenization mismatch: 260 vs. 262. #turn = 1. (ignored)
WARNING: tokenization mismatch: 289 vs. 291. #turn = 1. (ignored)
{'loss': 1.1816, 'learning_rate': 4.982671142387316e-05, 'epoch': 0.19}
  4%|â–         | 6/160 [10:20<4:02:04, 94.31s/it]WARNING: tokenization mismatch: 399 vs. 401. #turn = 1. (ignored)
WARNING: tokenization mismatch: 285 vs. 287. #turn = 1. (ignored)
{'loss': 1.626, 'learning_rate': 4.976423351108943e-05, 'epoch': 0.22}
  4%|â–         | 7/160 [11:55<4:01:47, 94.82s/it]WARNING: tokenization mismatch: 277 vs. 279. #turn = 1. (ignored)
...

...
{'loss': 0.0075, 'learning_rate': 1.2038183319507955e-07, 'epoch': 4.84}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 155/160 [4:06:18<07:21, 88.21s/it]WARNING: tokenization mismatch: 216 vs. 218. #turn = 1. (ignored)
WARNING: tokenization mismatch: 441 vs. 443. #turn = 1. (ignored)
{'loss': 0.0065, 'learning_rate': 7.706665667180091e-08, 'epoch': 4.88}
{'loss': 0.0056, 'learning_rate': 4.335974538210441e-08, 'epoch': 4.91}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 157/160 [4:09:15<04:23, 87.89s/it]WARNING: tokenization mismatch: 391 vs. 393. #turn = 1. (ignored)
{'loss': 0.005, 'learning_rate': 1.9274093981927478e-08, 'epoch': 4.94}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 158/160 [4:10:43<02:56, 88.25s/it]WARNING: tokenization mismatch: 270 vs. 272. #turn = 1. (ignored)
WARNING: tokenization mismatch: 432 vs. 434. #turn = 1. (ignored)
{'loss': 0.021, 'learning_rate': 4.818987948379539e-09, 'epoch': 4.97}
{'loss': 0.0079, 'learning_rate': 0.0, 'epoch': 5.0}
```

å¾®è°ƒåç»“æœï¼š

```
é—®: ç”Ÿæˆæ­£å¼ç”µå­é‚®ä»¶çš„ç»“æŸè¯­ã€‚
ç­”: [W OpCommand.cpp:117] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy1 (function operator())
ç”Ÿæˆæ­£å¼ç”µå­é‚®ä»¶çš„ç»“æŸè¯­éœ€è¦è€ƒè™‘å¤šä¸ªå› ç´ ï¼Œä¾‹å¦‚æ”¶ä»¶äººçš„èº«ä»½ã€é‚®ä»¶çš„ä¸»é¢˜ã€æ­£æ–‡å†…å®¹å’Œé™„åŠ ä¿¡æ¯ç­‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„ç»“æŸè¯­ç¤ºä¾‹ï¼š

- â€œæ„Ÿè°¢æ‚¨çš„å…³æ³¨å’Œæ”¯æŒï¼Œæˆ‘ä»¬å°†ç»§ç»­åŠªåŠ›ä¸ºæ‚¨æä¾›æ›´å¥½çš„æœåŠ¡ã€‚â€
- â€œå¸Œæœ›æ‚¨èƒ½å–œæ¬¢è¿™ä»½ç¤¼ç‰©ï¼Œå¹¶äº«å—æ‚¨çš„ç”Ÿæ´»ã€‚â€
- â€œæ„Ÿè°¢æ‚¨çš„å›å¤ï¼Œå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚â€
- â€œå¸Œæœ›æ‚¨èƒ½åœ¨è¿™ä¸ªå‡æœŸé‡Œåº¦è¿‡æ„‰å¿«çš„æ—¶å…‰ï¼Œå¹¶äº«å—ä¸å®¶äººå’Œæœ‹å‹çš„ç›¸å¤„ã€‚â€
â€œæ„Ÿè°¢æ‚¨å¯¹è¿™é¡¹å·¥ä½œçš„åŠªåŠ›ï¼Œæˆ‘ä»¬ç›¸ä¿¡æ‚¨ä¼šå–å¾—æˆåŠŸã€‚â€

è¿™äº›ç»“æŸè¯­éƒ½è¡¨è¾¾äº†ä¸€ç§æ„Ÿæ¿€ä¹‹æƒ…ï¼Œå¹¶å¼ºè°ƒäº†ä¸æ”¶ä»¶äººçš„è”ç³»å’Œå…³ç³»ã€‚å…·ä½“ä½¿ç”¨å“ªä¸ªç»“æŸè¯­å–å†³äºæ‚¨ä¸æ”¶ä»¶äººçš„å…³ç³»å’Œé‚®ä»¶çš„ä¸»é¢˜ã€‚åœ¨å‘é€ç”µå­é‚®ä»¶ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ£€æŸ¥æ‚¨çš„æ ¼å¼å’Œæ–‡æœ¬ï¼Œä»¥ç¡®ä¿é‚®ä»¶å‡†ç¡®ã€æ¸…æ™°å’Œæ˜“äºç†è§£ã€‚
```

å¾®è°ƒå‰ç»“æœï¼š

```
é—®: ç”Ÿæˆæ­£å¼ç”µå­é‚®ä»¶çš„ç»“æŸè¯­ã€‚
ç­”: [W OpCommand.cpp:117] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy1 (function operator())
å°Šæ•¬çš„[æ”¶ä»¶äººå§“å],

æ„Ÿè°¢æ‚¨æŠ½å‡ºå®è´µæ—¶é—´é˜…è¯»æˆ‘çš„é‚®ä»¶ã€‚æˆ‘çœŸè¯šåœ°å¸Œæœ›æ‚¨èƒ½å¤Ÿ[å…·ä½“å†…å®¹]ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®æˆ–éœ€è¦è¿›ä¸€æ­¥çš„ä¿¡æ¯ï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ã€‚

å†æ¬¡æ„Ÿè°¢æ‚¨çš„å…³æ³¨å’Œæ”¯æŒã€‚

ç¥å¥½ï¼Œ

[æ‚¨çš„å§“å]
```

--------

### é—®é¢˜

é—®é¢˜1ï¼š`ImportError: /root/miniconda/envs/torch_npu/bin/../lib/libgomp.so.1: cannot allocate memory in static TLS block`

è§£å†³æ–¹æ³•ï¼š`export LD_PRELOAD=/lib/aarch64-linux-gnu/libgomp.so.1`

é—®é¢˜2ï¼šç•Œé¢æ–¹å¼è¾“å…¥åæ— ååº”

è§£å†³æ–¹æ³•ï¼šå°† gradio ç‰ˆæœ¬è¿›è¡Œé™çº§ï¼Œ`pip install gradio==3.41.0`