
### 大语言模型

大语言模型一般指的是具有transformer结构的，使用大量数据训练出具有数亿参数量的模型。
大语言模型和小语言模型在模型结构和训练目标上区别不大，主要是在模型尺寸、训练数据量、算力上有极大的区别。

### 为什么是大模型

#### KM缩放定律（OpenAI）

![alt text](./images/image.png)

**模型参数量（N）**：非嵌入参数的数量。
**训练数据量（D）**：以token数量衡量。
**计算预算（C）**：训练过程中消耗的总浮点运算量（FLOPs）

**核心结论**：
模型性能强烈依赖三者：模型大小、数据量和计算量均显著影响损失，但数据量的边际收益最高
优化优先级：在资源有限时，扩大数据量可能比增加模型参数量更有效

#### Chinchilla定律（DeepMind）
KM是单变量分析，Chinchill是联合建模，并引入约束C~=6ND

![alt text](./images/image-1.png)

当优化固定计算预算C时，模型规模（N）和数据规模（D）应平衡分配

#### 涌现能力

**核心概念**：
涌现能力指小模型不具备，而大模型突然展现的能力，类似于物理学中的"相变"现象。当模型规模超过临界阈值时，性能会从随机水平突然跃升。

**关键特征**：
规模依赖性：仅在参数量达到一定规模（通常>100B）时出现。
任务复杂性关联：多见于需要多步推理、抽象理解或泛化的复杂任务。

### 大模型结构

**大模型发展**

![alt text](./images/image-2.png)

大模型在结构上可以分为以下几种：

**Encoder-Decoder 结构**

![alt text](./images/image-3.png)

**Encoder 结构**

![alt text](./images/image-5.png)

**Decoder 结构（gpt-2）**

![alt text](./images/image-4.png)

|特性|	Encoder-only|	Decoder-only|	Encoder-Decoder|
|---|---|---|---|
|注意力机制|	双向（全局上下文）|	单向（仅历史上下文）|	编码器双向+解码器单向|
|训练目标|	预测掩码词（MLM）|	预测下一个词|	序列到序列映射|
|典型输入输出|	单文本输入|	单文本输入/生成|	文本对输入输出（如翻译）|
|生成能力|	❌ 无	|✅ 强	|✅ 强（需解码器）|
|上下文理解|	✅ 强（双向）|	❌ 弱（单向）|	✅ 强（编码器双向）|
|代表模型|	BERT	|GPT-3|	T5|

### 大模型如何处理文本信息

#### BPE（Byte Pair Encoding）算法

https://tiktokenizer.vercel.app/

实现（https://github.com/openai/gpt-2/blob/master/src/encoder.py）

bpe训练阶段：
1. 全局统计所有文本的字节对频率。
2. 迭代合并最高频的字节对，生成合并规则（如 ("a", "t") → "at"）。
3. 将最终合并规则保存到 vocab.bpe 文件，同时保存词汇表encoder.json

![alt text](./images/image-7.png)

推理阶段：

![alt text](./images/image-8.png)

1. 分词：使用一定规则对文本进行切分
gpt2: `r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""`
2. 对分词后的单元进行bpe推理
    - 按照合并规则，逐级合并
    - 或者固定词汇表查询
3. 以上步骤完成text -> token 的转换
4. 大模型推理时，生成（batch_size, seq_len, vocab_size）的张量
5. 取概率最高的token id
6. 根据encoder.json转化为token








